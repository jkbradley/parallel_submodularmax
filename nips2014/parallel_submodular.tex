
\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}


% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{float}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{wrapfig}
\setlength{\emergencystretch}{3em}
\usepackage[numbers]{natbib}

\usepackage{thm-restate}


\usepackage{tikz}

\usepackage{multirow}

% For algorithms
\usepackage[algoruled,vlined,linesnumbered]{algorithm2e}
%\usepackage{algorithm}
%\usepackage{algorithmic}

\usepackage{multicol}
\usepackage{comment}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\graphicspath{{figures/}}


\newcommand{\ie}{{\em i.e.,}~}
\newcommand{\eg}{{\em e.g.,}~}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Algorithm names
\newcommand{\hogwild}{CF-2g}
\newcommand{\occ}{CC-2g}
\newcommand{\seqalg}{Seq-2g}
\newcommand{\hogwildshort}{CF}
\newcommand{\occshort}{CC}
\newcommand{\seqalgshort}{Seq}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begingroup
    \makeatletter
    \@for\theoremstyle:=definition,remark,plain\do{%
        \expandafter\g@addto@macro\csname th@\theoremstyle\endcsname{%
            \addtolength\thm@preskip\parskip
            }%
        }
\endgroup
\newtheorem{dfn}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{exmp}[thm]{Example}
\newtheorem{claim}{Claim}

\floatstyle{ruled}
\newfloat{program}{thp}{lop}
\floatname{program}{Program}

\newenvironment{denseitemize}{
\begin{itemize}[topsep=2pt, partopsep=0pt, leftmargin=1.5em]
  \setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}




\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
%  numbers=left,
  morestring=[b]"""
}



% Commenting system
\newcommand{\Comments}{1}
\newcommand{\note}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\xinghao}[1]{\note{red}{[XP: #1]}}
\newcommand{\joey}[1]{\note{blue}{[JG: #1]}}
\newcommand{\stef}[1]{\note{green}{[SJ: #1]}}
\newcommand{\joseph}[1]{\note{cyan}{[JB: #1]}}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}






%% ---------------------------------------------------------
%% Terminology
\newcommand{\term}[1]{\textbf{#1}}


%% ---------------------------------------------------------
%% Citation/Reference commands
\newcommand{\citecf}[1]{(\cf, \cite{#1})}
\newcommand{\tableref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\listref}[1]{Listing~\ref{#1}}

\newcommand{\eqnref}[1]{Eq.~(\ref{#1})}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\chapref}[1]{Chapter~\ref{#1}}

\newcommand{\dfnref}[1]{Definition~\ref{#1}}
\newcommand{\thmref}[1]{Thm.~\ref{#1}}
\newcommand{\propref}[1]{Prop.~\ref{#1}}
\newcommand{\lemref}[1]{Lem~\ref{#1}}
\newcommand{\exmpref}[1]{Example~\ref{#1}}
\newcommand{\corref}[1]{Cor.~\ref{#1}}
\newcommand{\algref}[1]{Alg.~\ref{#1}}
\newcommand{\procref}[1]{Proc.~\ref{#1}}
\newcommand{\alglineref}[1]{Line~\ref{#1}}
\newcommand{\probref}[1]{Problem~(\ref{#1})}
\newcommand{\appendref}[1]{Appendix~\ref{#1}}

%% ---------------------------------------------------------
%% Basic Math
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}


%% ---------------------------------------------------------
%% special math functions
\newcommand{\polylog}[2]{\,\mathbf{Li}_{#1}\left( #2 \right)}
\newcommand{\harmonic}[2]{\,\mathbf{h}_{#1}\left( #2 \right)}

%% ---------------------------------------------------------
%% Norms
\newcommand{\Lone}{L_{1}}
\newcommand{\Linf}{L_{\infty}}
\newcommand{\LInfNorm}[1]{\left|\left| #1 \right|\right|_{\infty}}
\newcommand{\LOneNorm}[1]{\left|\left| #1 \right|\right|_1}

\newcommand{\hinge}[1]{\left[  #1 \right]_+}

%% ---------------------------------------------------------
%% Probability notation
\newcommand{\given}{\,|\,}
\newcommand{\stdist}[1]{\mathbf{\pi} \left( #1 \right) }
\newcommand{\Prb}[1]{\mathbf{P} \left( #1 \right) }
\newcommand{\PrbEst}[1]{\mathbf{\tilde{P}} \left( #1 \right) }
\newcommand{\Ent}[1]{\mathbf{H} \left( #1 \right) }
\newcommand{\PiPrb}[1]{\Prb{ #1 } }
\newcommand{\Kern}[1]{K \left( #1 \right) }
\newcommand{\Ex}[1]{\mathbf{E} \left[ #1 \right] }
\newcommand{\Exwrt}[2]{\mathbf{E}_{#1} \left[ #2 \right] }
\newcommand{\Variance}[1]{\mathbf{Var} \left[ #1 \right] }
\newcommand{\Ind}[1]{\mathbf{1}\left[ #1 \right]}
\newcommand{\Bern}[1]{\text{Bern}( #1 ) }

%% ---------------------------------------------------------
%% Set notation
\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\vecspace}{\mathcal{V}}
\newcommand{\Union}{\bigcup}
\newcommand{\Inter}{\bigcap}
\newcommand{\union}{\cup}
\newcommand{\inter}{\cap}
\newcommand{\size}[1]{\left| #1 \right|}


%% ---------------------------------------------------------
%% Complexity
\newcommand{\BigO}[1]{O\hspace{-1pt}\left( #1 \right)}
\newcommand{\BigTheta}[1]{\Theta \left( #1 \right)}
\newcommand{\BigOmega}[1]{\Omega \left( #1 \right)}





%% ---------------------------------------------------------
%% Algorithms
\SetKwFor{ParForAll}{for}{do in parallel}{end}
\SetKwFunction{Map}{Map}
\SetKwFunction{Reduce}{Reduce}

\SetKwInput{Input}{Input}
\SetKwInput{Output}{Output}
\SetKwInput{SideEffect}{SideEffect}
\SetKwInput{Define}{Define}
\SetKwInput{Global}{Global}
\SetKwFor{DoWithProbability}{with probability}{}{}
\SetKwFunction{DPMeansOp}{DPMeansOp}
\SetKwFunction{DPValidate}{DPValidate}
\SetKwFunction{OFLValidate}{OFLValidate}
\SetKwFunction{BPMeansOp}{BPMeansOp}
\SetKwFunction{BPValidate}{BPValidate}
\SetKwFunction{NewClusters}{AcceptedClusters}

\SetKw{WaitUntil}{wait until}

\SetKwFunction{Mean}{Mean}
\SetKwFunction{Ref}{Ref}


%% ---------------------------------------------------------
%% Paper specific notation

% All the data
\newcommand{\data}{\mathcal{D}}
% \datablock{machine}
\newcommand{\datablock}[1]{\data_{#1}}

\newcommand{\clusters}{\mathcal{C}}
\newcommand{\gclusters}{\hat\clusters}
\newcommand{\newclusters}{\tilde\clusters}
% local clusters \lclusters{machine}
\newcommand{\lclusters}[1]{\clusters_{#1}}

%\newcommand{\bregd}[2]{D_\phi\left(#1,#2\right)}
\newcommand{\bregd}[2]{\left\|#1-#2\right\|}


% for ofl analysis:
\newcommand{\CFL}{C^{\text{FL}}}
\newcommand{\muFL}{\mu^{\text{FL}}}



\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother




\title{Parallel Double Greedy Submodular Maximization}

\author{
Xinghao Pan$^1$ Joseph Gonzalez$^1$ Stefanie Jegelka$^1$ Joseph Bradley$^{1}$ Michael I. Jordan$^{1,2}$\\
$^1$Department of Electrical Engineering and Computer Science, and $^2$Department of Statistics\\
University of California, Berkeley\\
Berkeley, CA USA 94720\\
  \texttt{\{xinghao,jegonzal,stefje,tab,?\}@eecs.berkeley.edu} \\
}

% \address{University of California
% 465 Soda Hall, MC-1776
% Berkeley, CA 94720-1776}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


%\nipsfinalcopy

\begin{document}


\maketitle


\begin{abstract}
Many machine learning problems can be reduced to the maximization of  submodular functions.
Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results \cite{Mirzasoleiman2013} only addressing monotone functions.
The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by \citet{buchbinder2012} and follows a strongly serial double-greedy logic and program analysis.
In this work, we propose two methods to parallelize the double-greedy algorithm.
% bridge this gap and make the theoretical benefits available in a scalable setting.
% % between those theoretical benefits and scalable computing.
% We propose and analyze two parallel double greedy algorithms.
The first, \emph{coordination-free} approach emphasizes speed at the cost of a weaker approximation guarantee.
The second, \emph{concurrency control} approach guarantees a tight 1/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism.
As a consequence we explore the trade off space between guaranteed performance and objective optimality.
% We bound both the weaker approximation factor and the reduction in parallelism.
We implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach.
%
%Many machine learning problems can be reduced to the maximization of a submodular function.
%Recently, \citet{buchbinder2012} achieved a tight 1/2-approximation for unconstrained submodular maximization using a double greedy algorithm.
%Unfortunately, the double greedy algorithm was developed and analyzed in the serial setting limiting our ability to leverage parallel hardware.
%In this work we propose and analyze two parallel extensions to the \cite{buchbinder2012} double greedy algorithm.
%The first, \emph{coordination-free} approach emphasizes speed at the cost of a weaker approximation guarantee.
%The second, \emph{concurrency control} approach guarantees the same tight 1/2-approximation, at the cost of additional coordination and reduced parallelism.
%We bound both the weaker approximation factor and the reduction in parallelism.
%We implement and evaluate both algorithms on multi-core hardware and billion edge graphs demonstrating both the scalability and tradeoffs of each approach.
%
% Many machine learning problems can be formulated as maximization of submodular functions. \cite{buchbinder2012} recently proposed a double greedy algorithm for unconstrained submodular maximization that achieves a tight 1/2-approximation. However, double greedy is an inherently sequential, linear time algorithm that does not scale to big data.

% We present two approaches to extend the double greedy algorithm to a parallel setting.
% The first, `coordination-free' approach emphasizes speed at the cost of a weaker approximation guarantee -- it achieves a $(1/2 - O(1/N))$ approximation for max cut on a complete graph.
% The second, `concurrency control' approach guarantees the same tight 1/2-approximation, at the cost of greater coordination.

% Our parallel algorithms scale well on synthetic and real datasets, and suffer little or no loss of objective value compared to the sequential algorithm.
\end{abstract}

\section{Introduction}



Many important problems including sensor placement \cite{krauseGuestrin11}, image co-segmentation \cite{kim11}, MAP inference for determinantal point processes \cite{gillenwater12}, influence maximization in social networks \cite{kkt03}, and document summarization \cite{lin11} may be expressed as the maximization of a submodular function.
The submodular formulation enables the use of targeted algorithms \cite{buchbinder2012,nemhauser1978} that offer theoretical worst-case guarantees on the quality of the solution.
For several maximization problems of \emph{monotone} submodular functions (satisfying $F(A) \leq F(B)$ for all $A \subseteq B$), a simple greedy algorithm \cite{nemhauser1978} achieves the optimal approximation factor of $1-\frac{1}{e}$.
The optimal result for the wider, important class of \emph{non-monotone} functions  -- an approximation guarantee of $1/2$ --  is much more recent, and achieved by a \emph{double greedy} algorithm by \citet{buchbinder2012}.

While theoretically optimal, in practice these algorithms do not scale to large real world problems.
The inherently serial nature of the algorithms poses a challenge to leveraging advances in parallel hardware.
This limitation raises the question of parallel algorithms for submodular maximization that ideally preserve the theoretical bounds, or weaken them gracefully, in a quantifiable manner.


% While these algorithms are both theoretically optimal and perform well in practice, their design is inherently serial, limiting their scalability to large problem instances and advances in parallel hardware.
% % Increasingly often, real-world applications demand
% % % one wishes
% % to solve the above-mentioned problems at very large scale.
% This limitation raises the question of parallel algorithms for submodular maximization that ideally preserve the theoretical bounds, or weaken them gracefully, in a quantifiable manner.


In this paper, we address the challenge of parallelization of greedy algorithms, in particular the double greedy algorithm, from the perspective of \emph{parallel transaction processing systems}.
This alternative perspective allows us to apply advances in database research ranging from fast coordination free approaches with limited guarantees to sophisticated concurrency control techniques which ensure a direct correspondence between parallel and serial executions at the expense of increased coordination. % cost of slightly reduced performance.


We develop two parallel algorithms for the maximization of non-monotone submodular functions that operate at different points along the coordination tradeoff curve.
We propose \hogwild{} as a coordination free
%double greedy
algorithm and characterize the effect of reduced coordination on the approximation ratio.
By bounding the possible outcomes of concurrent transactions we introduce the \occ{} algorithm which guarantees serializable parallel execution and retains the optimality of the double greedy algorithm at the expense of increased coordination.
% By exploiting the exchangeability of the greedy ordering and the sparsity of  the submodular functions we develop two parallel algorithms for the maximization of non-monotone submodular functions.
% We propose \hogwild{} as a coordination free double greedy algorithm and characterize the effect of reduced coordination on the approximation ratio.
% By bounding the possible outcomes of concurrent transactions we introduce the \occ{} algorithm which guarantees serializable parallel execution and retains the optimality of the double greedy algorithm at the expense of increased coordination.
The primary contributions of this paper are:
\begin{enumerate}
\item We propose two parallel algorithms for unconstrained non-monotone submodular maximization, which trade off parallelism and tight approximation guarantees.
\item We provide approximation guarantees for \hogwild{} and analytically bound the expected loss in objective value for set-cover and max-cut.
\item We prove that \occ{} preserves the optimality of the serial double greedy algorithm and analytically bound the additional coordination overhead for set-cover and max-cut.
% an outcome equivalent to the sequential double greedy algorithm, thus preserving the tight expected 1/2-approximation guarantee.
% \item We analytically bound the amount of coordination of \occ{} for set-cover and max-cut.
\item We demonstrate empirically using two synthetic and four real datasets that our parallel algorithms perform well in terms of both speed and objective values.
\end{enumerate}

The rest of the paper is organized as follows.
\secref{sec:submodularmax} discusses the problem of submodular maximization and introduces the double greedy algorithm.
\secref{sec:concurrencycontrol} provides background on concurrency control mechanisms.
We describe and provide intuition for our \hogwild{} and \occ{} algorithms in \secref{sec:alghogwild} and \secref{sec:algocc}, and then analyze the algorithms both theoretically (\secref{sec:analysis}) and empirically (\secref{sec:evaluation}).
%\secref{sec:analysis} analyzes the parallel algorithms.
%Both algorithms are evaluated in \secref{sec:evaluation}, and we conclude with related work and discussions in \secref{sec:related} and \secref{sec:discussions}.














\section{Submodular Maximization \label{sec:submodularmax}}
A set function $F: 2^V \to \mathbb{R}$ defined over subsets of a ground set $V$ is \emph{submodular} if it satisfies \emph{diminishing marginal returns}: for all $A \subseteq B \subseteq V$ and  $e \notin V$, it holds that $F(A \union \{e\}) - F(A) \geq F(B \union \{e\}) - F(B)$. Throughout this paper, we will assume that $F$ is nonnegative and $F(\emptyset)=0$.
Submodular functions have emerged in areas such as game theory \cite{shapley71}, graph theory \cite{frank93}, combinatorial optimization \cite{schrijver02}, %stochastic processes \cite{},
and machine learning \cite{tutorial,tutorial2}.
Casting machine learning as submodular optimization enables the use of algorithms for submodular maximization \cite{buchbinder2012,nemhauser1978} that offer theoretical worst-case guarantees on the quality of the solution.
%Problems like as sensor placement \cite{krauseGuestrin11}, image co-segmentation \cite{kim11}, MAP inference with determinantal point process priors \cite{gillenwater12}, influence maximization \cite{kkt03} or document summarization \cite{lin11} may be phrased as the maximization of a submodular function, which enables the use of algorithms for submodular maximization \cite{buchbinder2012,nemhauser1978} that offer theoretical worst-case guarantees on the quality of the solution.

While those algorithms confer strong guarantees, their design is inherently serial, limiting their usability in large-scale problems.
% But increasingly often, one wishes to solve the above-mentioned problems at very large scale. This development raises the question of parallel algorithms for submodular maximization that ideally preserve the theoretical bounds, or weaken them gracefully, in a quantifiable manner.
%
Recent work has addressed faster \cite{badan14,wei14} and parallel \cite{Mirzasoleiman2013,kumar13} versions of the greedy algorithm by \citet{nemhauser1978} for maximizing \emph{monotone} submodular functions that satisfy $F(A) \leq F(B)$ for any $A \subseteq B \subseteq V$.
However, many important applications in machine learning lead to \emph{non-monotone} submodular functions.
For example, graphical model inference \cite{gillenwater12,reed13}, or trading off any submodular gain maximization with costs (functions of the form $F(S) = G(S) - \lambda M(S)$, where $G(S)$ is monotone submodular and $M(S)$ a linear (modular) cost function), such as for utility-privacy tradeoffs \cite{krause10priv},
%and feature selection with a modular cost function \joey{Is there a better example or at least a citation?}
 require maximizing non-monotone submodular functions.
% and when trading off gains with (linear) cost functions (functions of the form $F(S) = G(S) + \lambda M(S)$, where $G(S)$ is monotone submodular and $M(S)$ is a linear (modular) cost function), we aim to maximize a \emph{non-monotone} submodular function.
For non-monotone functions, the simple greedy algorithm in \cite{nemhauser1978} can perform arbitrarily poorly (see Appendix~\ref{app:greedyfail} for an example).
Intuitively, the introduction of additional elements with monotone submodular functions never decreases the objective while introducing elements with non-monotone submodular functions can \emph{decrease} the objective to its minimum.
% Intuitively, for monotone functions including more elements eventually neither helps nor hurts, while for non-monotone functions it can hurt to the extent of reducing the function value back to zero.
For non-monotone functions, \citet{buchbinder2012} recently proposed an optimal double greedy algorithm that works well in a serial setting. In this paper, we study parallelizations of this algorithm.

% they develop and analyze in a serial setting. Here, we theoretically and empirically study parallel analogs of this algorithm.

\paragraph{The sequential double greedy algorithm.}
The sequential double greedy algorithm of \citet{buchbinder2012} (\seqalg{}, in \algref{alg:submax}) maintains two sets $A^i \subseteq B^i$.
Initially, $A^0 = \emptyset$ and $B^0 = V$.
In iteration $i$, the set $A^{i-1}$ contains the items selected before item/iteration $i$, and $B^{i-1}$ contains $A^i$ and the items that are so far undecided.
The algorithm sequentially passes through the items in $V$ and determines online whether to keep item $i$ (add to $A^i$) or discard it (remove from $B^i$), based on a threshold that trades off the gain $\Delta_+(i) = F(A^{i-1} \union i) - F(A^{i-1})$ of adding $i$ to the currently selected set $A^{i-1}$, and the gain $\Delta_-(i) = F(B^{i-1}\setminus i) - F(B^{i-1})$ of removing $i$ from the candidate set, estimating its complementarity to other remaining elements.
% The decision depends on the gain of $i$ with respect to $A^i$, and the gain of removing $i$ from $B^i$.
% If the submodular function is monotone, then the double greedy algorithm essentially becomes a randomized version of the well-known greedy algorithm for monotone functions \citep{nemhauser1978}.
%Describe the double greedy algorithm:
%The sequential double greedy \cite{buchbinder2012} algorithm monotonically grows $A^i$ and shrinks $B^i$.









\section{Concurrency Patterns for Parallel Machine Learning \label{sec:concurrencycontrol}}

In this paper we adopt a transactional view of the program state and explore parallelization strategies through the lens of parallel transaction processing systems.
We recast the program state, the sets $A$ and $B$, as data, and the operations, adding elements to $A$ and removing elements from $B$, as transactions.
More precisely we reformulate the double greedy algorithm (\algref{alg:submax}) as a series of \emph{exchangeable}, \emph{Read-Write} transactions of the form:
\begin{equation}
T_e(A,B) \triangleq
\begin{cases}
   (A \union e, B) & \text{if } u_e \leq \frac{\hinge{\Delta_+(A,e)}}{ \hinge{\Delta_+(A,e)} + \hinge{\Delta_-(B,e)}}  \\
   (A, B \backslash e) & \text{otherwise. }
  \end{cases}
  \label{eqn:greedytransaction}
\end{equation}
The transaction $T_e$ is a function from the sets $A$ and $B$ to new sets $A$ and $B$ based on the element $e \in V$ and the predetermined random bits $u_e$ for that element.
To eliminate exogenous ordering effects, we associate the source of randomness with each element in the set.
% to simplify the presentation. % as $T_e$ becomes a deterministic function.


By composing the transactions $T_n (T_{n-1}(\ldots T_1(\emptyset, V)))$ we recover the serial double-greedy algorithm defined in \algref{alg:submax}.
In fact, any ordering of the \emph{serial} composition of the transactions recovers a permuted execution of \algref{alg:submax} and therefore the optimal approximation algorithm.
However, this raises the question: \emph{is it possible to apply transactions in parallel?}
If we execute transactions $T_i$ and $T_j$, with $i \neq j$, in parallel we
need a method to merge the resulting program states.
In the context of the double greedy algorithm, we could define the parallel execution of two transactions as:
\begin{equation}
T_i(A,B) + T_j(A,B) \triangleq \left(T_i(A,B)_A \union T_j(A,B)_A,  \,\, T_i(A,B)_B  \inter T_j(A,B)_B \right).
\label{eqn:merge}
\end{equation}
the union of the resulting $A$ and the intersection of the resulting $B$.
While we can easily generalize \eqnref{eqn:merge} to many parallel transactions, we cannot always guarantee that the result will correspond to a serial composition of transactions.
As a consequence, we cannot directly apply the analysis of Buchbinder et al.~\cite{buchbinder2012} to derive strong approximation guarantees for the parallel execution.

Fortunately, several decades of research~\citep{Ozsu07,kung1981:occ} in database systems have explored efficient parallel transaction processing.
In this paper we adopt a bounded optimistic approach to parallel transaction processing in which parallel transactions are constructed under bounds on the possible program state.
If the transaction could violate the bound then it is processed serially on the server.
By adjusting the definition of the bound we can span a space of coordination free to serializable executions.




% These systems span a space ranging from extremely fast coordination free approaches which provide minimal guarantees on the composition of transactions to sophisticated concurrency control techniques which ensure a direct correspondence between parallel and serial executions at the cost of slightly reduced performance.




% A common technique is to introduce a validation stage that detects inconsistent transactions and prescribes a compensating action.
% In \algref{alg:generalparallel} we describe a general validation based meta-algorithm that we extend in subsequent sections.
% Each processor extracts a next element to process and a consistent but potentially out of data snapshot of the program state.
% Based on the snapshop and selected element, each processor proposes a transaction, $\partial_e$, and the preconditions under which the transaction is valid, $\mathfrak{A}$.
% We denote the transaction as $\partial_e$ to avoid confusion with \eqnref{eqn:greedytransaction} and because it encodes the effect (\eg remove $e$ from $B$) of more costly transactions (\eg \eqnref{eqn:greedytransaction}).


% After proposing a transaction and corresponding preconditions, the processor validates the transaction by invoking the validation process defined in \algref{alg:generalparallel:validate}.
% In this work we restrict our attention to an atomic validation process.
% The validation function ensures that the proposed transaction hasn't already failed and that the preconditions are satisfied.
% If the transaction is invalid, then a compensating action is taken.
% Here we consider simple compensating actions in which a valid transaction is constructed.
% Finally, the valid transaction is applied advancing the program state.


% By changing the preconditions and proposal process we can span a range of potential parallelization strategies.
% By weakening the preconditions we can minimized or even eliminate invalid transactions and the need for validation.
% Alternatively, by carefully constructing the preconditions we can guarantee a serializable execution potentially at the expense of parallel scalability.




\begin{figure}[h]
  \footnotesize
  \centering
  \begin{multicols}{2}
    \begin{minipage}{0.45\textwidth}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{Generalized transactions}
        \label{alg:generalparallel}
        \ParForAll{$p \in \set{1, \ldots, P}$}{
          \While{$\exists$ element to process}{
            $e = $ next element to process\;
            $(\mathfrak{g}_e, i) = \text{requestGuarantee}(e)$\;
            $\partial_i$ = propose($e$, $\mathfrak{g}_e$)\;
            commit($e$, $i$, $\partial_i$) \tcp{Non-blocking}
          }
        }
      \end{algorithm}

    \end{minipage}

    \begin{minipage}{0.45\textwidth}
      \begin{algorithm}[H]
        \SetKwBlock{Atomically}{Atomically}{}
        \DontPrintSemicolon
        \caption{Commit}
        \label{alg:generalparallel:commit}
        \WaitUntil $\forall j < i$, processed$(j) = true$\;
        \Atomically{
        \If{$\partial_i =\text{FAIL}$}{
          \tcp{Deferred proposal}
          $\partial_i$ = propose($e$, $\mathfrak{S}$)\;
        }
        \tcp{Advance the program state}
        $\mathfrak{S} \leftarrow \partial_i(\mathfrak{S})$\;
        }
      \end{algorithm}
    \end{minipage}
  \end{multicols}
  \vspace{-1em}
  \caption{\footnotesize  Algorithm for generalized transactions. Each transaction requests for its position $i$ in the commit ordering, as well as the bounds $\mathfrak{g}_e$ that are guaranteed to hold when it commits. Transactions are also guaranteed to be committed according to the given ordering.}
  \label{fig:transactionmodel}
\end{figure}


In \figref{fig:transactionmodel} we describe the bounded optimistic transaction pattern.
% \xinghao{I think this pattern fits pessimistic better than optimistic, since we're obtaining / releasing guarantees, a la read/write locks in DBMSes.}
The clients (\algref{alg:generalparallel}), in parallel, construct and commit transactions under bounded assumptions about the program state $\mathfrak{S}$ (\ie the sets $A$ and $B$).
Transactions are constructed by requesting the latest bound $\mathfrak{g}_e$ on $\mathfrak{S}$
% the program state
at logical time $i$ and computing a change $\partial_i$ to $\mathfrak{S}$
%the program state
(\eg Add $e$ to A).
If the bound is insufficient to construct the transaction then $\partial_i = \text{FAIL}$ is returned.
The client then sends the proposed change $\partial_i$ to the server to be committed atomically and proceeds to the next element without waiting for a response.

% begin to construct transaction begins by requesting a set of guarantees $\mathfrak{g}_e$ and an associated logical time $i$, such that the guarantee $\mathfrak{g}_e$ holds at time $i$.
% Under these conditions, the transaction attempts to propose and commit an update.
% However, if the guaranteed conditions are insufficient to do so, the transaction fails locally and the computation needs to be deferred to the server where it will have atomic access to the known true state $\mathfrak{S}$.
% \xinghao{We could add a footnote here that states that an alternative approach which we do not discuss in detail is for the transaction to rollback and re-try.}

The server (\algref{alg:generalparallel:commit}) \emph{serially} applies the transactions advancing the program state (\ie adding elements to $A$ or removing elements from $B$).
If the bounds were insufficient and the transaction failed at the client (\ie $\partial_i = \text{FAIL}$) then the server \emph{serially} reconstruct and applies the transaction under the true program state.
Moreover, the server is responsible for deriving bounds, processing transactions in the logical order $i$, and producing the serializable output $\partial_n (\partial_{n-1}(\ldots \partial_1(\mathfrak{S})))$.
% A parallel algorithm is said to be \emph{serializable}
% if for any input, the output of the parallel algorithm corresponds to that of some sequential execution.


% The server is in charge of providing and guaranteeing the conditions, and processing commits in the logical order of $i$, producing the output $\partial_n (\partial_{n-1}(\ldots \partial_1(\mathfrak{S})))$.
% A parallel algorithm is said to be \emph{serializable}
% %to a sequential algorithm
% if for any input, the output of the parallel algorithm corresponds to that of some sequential execution.

% is in charge of providing and guaranteeing the conditions, and processing commits in the logical order of $i$, producing the output $\partial_n (\partial_{n-1}(\ldots \partial_1(\mathfrak{S})))$.


This model achieves a high degree of parallelism when the cost of constructing the transaction dominates the cost of applying the transaction.
For example, in the case of submodular maximization, the cost of constructing the transaction depends on evaluating the marginal gains with respect to changes in $A$ and $B$ while the cost of applying the transaction reduces to setting a bit.
% Thus, distributing the work of proposals over multiple threads allows database systems to achieve parallelism, even with the strongly serial commit process on the server.
It is also essential that only a few transactions fail at the client.
Indeed, the analysis of these systems focuses on ensuring that the majority of the transactions succeed.
%Crucially, having a small number of deferred proposals, leaving the server to mainly perform the lightweight advancement of program state.







%\Joe scratch space.

%  correspond to preconditions

% necessary to maintain an invariant based notion of validity which departs fro more classic validation models.

%  conditional notion validity is a departure from more classic validation models and more closely resembles the notion of

% Finally, a validation function is called which verifies that the conditions $\mathfrak{S}$ are still valid with the current global state and if so atomically applies the operation

% Inspired by the work of Xinghao et al.~\cite{Xinghao13} we adopt a

% In this paper we propose and compare two techniques.  The simplest, inspired by Recht et al.~\cite{Recht11} is a coordination free approach which provides weak guarantees.

% towards coordination free approaches

% several techniques from this literature and propose both a coordination free approach as well as an approach based on optimistic concurrency control
% to parallel transaction processing with an optimistic concurrency control approach inspired by the recent work of


% \subsection{Coordination free}
% The coord

% \joey{finish}
% Simply run everything in parallel.
% Optimized for speed, but does not necessarily provide the correct answer.
% Requires work to prove correctness.

% \subsection{Concurrency control}
% Ensures `serial equivalence' -- the outcome of the parallel algorithm is equivalent to some execution of the sequential algorithm.
% Locally, threads take actions that are guaranteed to be safe (i.e. preserves serial equivalence), and forces additional coordination only when they are unable to execute their action safely.
% Designed for correctness, but requires coordination that compromises speed.
% Work is only required to demonstrate that coordination is limited.





\begin{figure}[t]
  \footnotesize
  \centering
  \begin{multicols}{2}
    \begin{minipage}{0.47\textwidth}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\seqalg{}: Serial double greedy}
        \label{alg:submax}
        %\Input{}
        $A^0 = \emptyset$, $B^0 = V$\;
        \For{$i = 1$ to $n$}{
          $\Delta_{+}(i) = F(A^{i-1}\cup i) - F(A^{i-1})$\;
          $\Delta_{-}(i) = F(B^{i-1}\backslash i) - F(B^{i-1})$\;
          Draw $u_i\sim Unif(0,1)$\;
          \If {$u_i<\frac{\hinge{\Delta_{+}(i)}}{\hinge{\Delta_{+}(i)} + \hinge{\Delta_{-}(i)}}$}{
            $A^i := A^{i-1} \cup i$;
            $B^i := B^{i-1}$\;
          }\lElse{
            $A^i := A^{i-1}$;
            $B^i := B^{i-1}\backslash i$
          }
        }
        %\Output{$A_n$}
      \end{algorithm}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\hogwild{}: coord-free double greedy}
        \label{alg:hogwild}
        $\hat{A} = \emptyset$, $\hat{B} = V$\;
        % \lFor{$e\in V$}{$\hat{A}(e) = 0$, $\hat{B}(e) = 1$}
        \ParForAll{$p \in \set{1, \ldots, P}$}{
          \While{$\exists$ element to process}{
            $e = $ next element to process\;
            $\hat{A}_e = \hat{A}$; $\hat{B}_e = \hat{B}$\;
            $\Delta_{+}^{\max}(e) = F(\hat{A}_e\cup e) - F(\hat{A}_e)$\;\label{alg:hogwild:deltaadd}
            $\Delta_{-}^{\max}(e) = F(\hat{B}_e\backslash e) - F(\hat{B}_e)$\;\label{alg:hogwild:deltarem}
            Draw $u_e\sim Unif(0,1)$\;\label{alg:hogwild:time}
            \If {$u_e<\frac{[\Delta_{+}^{\max}(e)]_+}{[\Delta_{+}^{\max}(e)]_+ + [\Delta_{-}^{\max}(e)]_+}$}{
              $\hat{A}(e) \leftarrow 1$\;\label{alg:hogwild:add}
            }\lElse{
              $\hat{B}(e) \leftarrow 0$\label{alg:hogwild:rem}
            }
          }
        }
      \end{algorithm}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\occ{}: concurrency control}
        \label{alg:occ}
        $\hat{A} = \tilde{A} = \emptyset$, $\hat{B} = \tilde{B} = V$\;
        % \lFor{$e\in V$}{$\hat{A}(e) = \tilde{A}(e) = 0$, $\hat{B}(e) = \tilde{B}(e) = 1$}
        % \lFor{$i = 1,\dots,|V|$}{result$(i) = 0$}
        \lFor{$i = 1,\dots,|V|$}{processed$(i) = false$}
        $\iota = 0$\;
        \ParForAll{$p \in \set{1, \ldots, P}$}{
          \While{$\exists$ element to process}{
            $e = $ next element to process\;
            $(\hat{A}_e, \tilde{A}_e, \hat{B}_e, \tilde{B}_e, i)$ = getGuarantee($e$)\;
            (result, $u_e$) = propose($e$, $\hat{A}_e$, $\tilde{A}_e$, $\hat{B}_e$, $\tilde{B}_e$)\;
            commit($e$, $i$, $u_e$, result)
          }
        }
      \end{algorithm}


    \end{minipage}

    \begin{minipage}{0.47\textwidth}
      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\occ{} getGuarantee($e$)}
        \label{alg:occsnapshot}
        $\tilde{A}(e) \leftarrow 1$;
        $\tilde{B}(e) \leftarrow 0$\;
        $i = \iota$;
        $\iota \leftarrow \iota + 1$\;\label{alg:occ:time}
        $\hat{A}_e = \hat{A}$;
        $\hat{B}_e = \hat{B}$\;
        $\tilde{A}_e = \tilde{A}$;
        $\tilde{B}_e = \tilde{B}$\;
        \Return $(\hat{A}_e, \tilde{A}_e, \hat{B}_e, \tilde{B}_e, i)$
      \end{algorithm}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\occ{} propose}
        \label{alg:propose}
        $\Delta_+^{\min}(e) = F(\tilde{A}_e) - F(\tilde{A}_e \backslash e)$\;\label{alg:occ:deltaplusmin}
        $\Delta_+^{\max}(e) = F(\hat{A}_e   \cup e) - F(\hat{A}_e)$\;\label{alg:occ:deltaplusmax}
        $\Delta_-^{\min}(e) = F(\tilde{B}_e) - F(\tilde{B}_e \cup e)$\;\label{alg:occ:deltaminusmin}
        $\Delta_-^{\max}(e) = F(\hat{B}_e   \backslash e) - F(\hat{B}_e)$\;\label{alg:occ:deltaminusmax}
        Draw $u_e \sim Unif(0,1)$\;
        \If {$u_e < \frac{[\Delta_+^{\min}(e)]_+}{[\Delta_+^{\min}(e)]_+ + [\Delta_-^{\max}(e)]_+}$}{\label{alg:occ:decisioninclude}
          result$~\leftarrow 1$
        }\ElseIf {$u_e > \frac{[\Delta_+^{\max}(e)]_+}{[\Delta_+^{\max}(e)]_+ + [\Delta_-^{\min}(e)]_+}$}{
          result$~\leftarrow -1$\;
        }\lElse{result$~\leftarrow \text{FAIL}$}
        \Return (result, $u_e$)
      \end{algorithm}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\occ{}: commit($e$, $i$, $u_e$, result)}
        \label{alg:occcommit}
        % \WaitUntil $\forall j<i$, result$(j) \neq 0$\;\label{alg:occ:resultwait}
        \WaitUntil $\forall j < i$, processed$(j) = true$\;\label{alg:occ:processedwait}
        \If{result$~= \text{FAIL}$}{
          $\Delta_+^{\text{exact}}(e) = F(\hat{A} \cup e) - F(\hat{A})$\;\label{alg:occcommit:deltaplus}
          $\Delta_-^{\text{exact}}(e) = F(\hat{B} \backslash e) - F(\hat{B})$\;\label{alg:occcommit:deltaminus}
          \lIf {$u_e < \frac{[\Delta_+^{\text{exact}}(e)]_+}{[\Delta_+^{\text{exact}}(e)]_+ + [\Delta_-^{\text{exact}}(e)]_+}$}{\label{alg:occcommit:decisioninclude}
            result$~\leftarrow 1$
          }\lElse{
            result$~\leftarrow -1$
          }
        }
        \lIf {result$~= 1$}{
          $\hat{A}(e)   \leftarrow 1$;\label{alg:occ:ahat}
          $\tilde{B}(e) \leftarrow 1$
        }\lElse{
          $\tilde{A}(e) \leftarrow 0$;
          $\hat{B}(e)   \leftarrow 0$\label{alg:occ:bhat}
        }
        processed$(i) = true$\;
      \end{algorithm}


    \end{minipage}



  \end{multicols}
  \label{fig:submax}
  \vspace{-1em}
\end{figure}




\section{Coordination Free Double Greedy Algorithm \label{sec:alghogwild}}
The coordination-free approach attempts to reduce the need to coordinate guarantees and logical ordering.
This is achieved by operating on potentially stale states -- the guarantee reduces to requiring $\mathfrak{g}_e$ be a stale version of $\mathfrak{S}$, and logical ordering is implicitly defined by the time of commit.
In using these weak guarantees, \hogwild{} is overly optimistically assuming that concurrent transactions are independent, which could potentially lead to erroneous decisions.
% \xinghao{An interesting take on hogwild -- hogwild is not serially equivalent to the sequential algorithm, but it is serially equivalent to an approximate algorithm that operates on stale states.
% Thus, to understand hogwild, we analyze the approximate algorithm.
% The concept of CC buys us two things: firstly, to extend sequential algorithms into parallel ones; secondly, to project a parallel algorithm into a sequential algorithm, which aids our understanding and analysis.}

\algref{alg:hogwild} is the coordination free parallel double greedy algorithm.\footnote{We present only the parallelized probabilistic versions of \cite{buchbinder2012}.
Both parallel algorithms can be easily extended to the deterministic version of \cite{buchbinder2012}; \hogwild{} can also be extended to the multilinear version of \cite{buchbinder2012}.\label{fn:extension}}
\hogwild{} closely resembles the serial \seqalg{}, but the elements $e \in V$ are no longer processed in a fixed order.
Thus, the sets $A, B$ are replaced by potentially stale local estimates (bounds) $\hat{A}, \hat{B}$, where $\hat{A}$ is a subset of the true $A$ and $\hat{B}$ is a superset of the actual $B$ on each iteration.
These bounding sets allow us to compute bounds $\Delta_{+}^{\max}, \Delta_{-}^{\max}$ which approximate $\Delta_{+}, \Delta_{-}$ from the serial algorithm.
We now formalize this idea.

To analyze the \hogwild{} algorithm we order the elements $e \in V$ according to the commit time (\ie when \algref{alg:hogwild} line \ref{alg:hogwild:time} is executed).
Let $\iota(e)$ be the position of $e$ in this total ordering on elements.
This ordering allows us to define monotonically non-decreasing sets $A^i = \{e' : e' \in A, \iota(e') < i\}$ where $A$ is the final returned set, and monotonically non-increasing sets $B^i = A^i \cup \{e': \iota(e') \geq i\}$.
The sets $A^i, B^i$ provide a serialization against which we can compare \hogwild{}; in this serialization, \algref{alg:submax} computes
$\Delta_{+}       (e) = F(A^{\iota(e)-1}\cup e) - F(A^{\iota(e)-1})$
and
$\Delta_{-}       (e) = F(B^{\iota(e)-1}\backslash e) - F(B^{\iota(e)-1})$.
% \begin{align*}
%   \Delta_{+}       (e) &= F(A^{\iota(e)-1}\cup i) - F(A^{\iota(e)-1}),
% & \Delta_{-}       (e) &= F(B^{\iota(e)-1}\backslash e) - F(B^{\iota(e)-1}) \, .
% \end{align*}
On the other hand, \hogwild{} uses stale versions\footnote{
  For clarity, we present the algorithm as creating a copy of $\hat{A}$, $\hat{B}$, $\tilde{A}$, and $\tilde{B}$ for each element. In practice, it is more efficient to update and access them in shared memory. Nevertheless, our theorems hold for both settings.
  \label{fn:copyvsshared}
}
$\hat{A}_e$, $\hat{B}_e$:
\algref{alg:hogwild} computes
$\Delta_{+}^{\max}(e) = F(\hat{A}_e\cup e) - F(\hat{A}_e)$
and
$\Delta_{-}^{\max}(e) = F(\hat{B}_e\backslash e) - F(\hat{B}_e)$.
% \begin{align*}
%   \Delta_{+}^{\max}(e) &= F(\hat{A}_e\cup e) - F(\hat{A}_e),
% & \Delta_{-}^{\max}(e) &= F(\hat{B}_e\backslash e) - F(\hat{B}_e) \, .
% \end{align*}

The next lemma shows that $\hat{A}_e, \hat{B}_e$ are bounding sets for the serialization's sets $A^{\iota(e)-1}, B^{\iota(e)-1}$.
Intuitively, the bounds hold because $\hat{A}_e$, $\hat{B}_e$ are stale versions of $A^{\iota(e)-1}$, $B^{\iota(e)-1}$, which are monotonically non-decreasing and non-increasing sets.
Full details of proof are given in Appendix \ref{app:algoproof}.

\begin{restatable}{lem}{lemhogsetbound}\label{lem:hog:set_bound}
In \hogwild{}, for any $e\in V$, $\hat{A}_e \subseteq A^{\iota(e)-1}$, and $\hat{B}_e \supseteq B^{\iota(e)-1}$.
\end{restatable}

\begin{cor}\label{cor:hog:delta_bound}
Submodularity of $F$ implies for \hogwild{}
$\Delta_{+}(e) \leq \Delta_{+}^{\max}(e)$, and
$\Delta_{-}(e) \leq \Delta_{-}^{\max}(e)$.
\end{cor}

The error in \hogwild{} depends on the tightness of the bounds in \corref{cor:hog:delta_bound}.
We analyze this in \secref{sec:analysis:hogwild}.

% For some functions $F$, we can maintain sketches or statistics to aid the computation of $\Delta_+^{\max}$, $\Delta_-^{\max}$, and still obtain the bounds given in Corollary \ref{cor:hog:delta_bound}.
% In Appendix \ref{sec:sepsum}, we consider functions of separable sums, which are useful for applications such as document summarization \cite{lin11}.
%we consider functions of the form
%$F(X) = \sum_{l=1}^L g\left(\sum_{i\in X\cup S_l} w_l(i)\right) - \lambda\sum_{i\in X} v(i)$,
%where $S_l \subseteq V$ are (possibly overlapping) groups of elements in the ground set, $g$ is a non-decreasing concave scalar function, and $w_l(i)$ and $v(i)$ are non-negative scalar weights.





\begin{figure}[t]
  \centering
  \begin{tabular}{cccc}
    \begin{subfigure}[h]{0.30\textwidth}
      \centering
      \includegraphics[width=110pt]{images/probvec_seq.pdf}
      \label{fig:probvecseq}
      \caption{\footnotesize\seqalg{}}
    \end{subfigure} &
    \begin{subfigure}[h]{0.30\textwidth}
      \centering
      \includegraphics[width=110pt]{images/probvec_CF.pdf}
      \label{fig:probveccf}
      \caption{\footnotesize\hogwild{}}
    \end{subfigure} &
    \begin{subfigure}[h]{0.30\textwidth}
      \centering
      \includegraphics[width=110pt]{images/probvec_CC.pdf}
      \label{fig:probveccc}
      \caption{\footnotesize\occ{}}
    \end{subfigure}
  \end{tabular}
  \caption{\footnotesize Illustration of algorithms.
  \textbf{(a)} \seqalg{} computes a threshold based on the true values $\Delta_+$, $\Delta_-$, and chooses an action based by comparing a uniform random $u_i$ against the threshold.
  \textbf{(b)} \hogwild{} approximates the threshold based on stale $\hat{A}$, $\hat{B}$, possibly choosing the wrong action.
  \textbf{(c)} \occ{} computes two thresholds based on the bounds on $A$, $B$, which defines an uncertainty region where it is not possible to choose the correct action locally.  If the random value $u_e$ falls inside the uncertainty interval than the transaction FAILS and must be recomputed serially by the server otherwise the transaction holds under all possible global states. % at the time the transaction will be processed.
  }
\end{figure}





\section{Concurrency Control for Double Greedy Algorithm \label{sec:algocc}}
% \xinghao{Provide more intuition for what \occ{} is doing.}

The concurrency-control based double greedy algorithm\footnoteref{fn:extension}, \occ{}, is presented in \algref{alg:occ}, and closely follows the meta-algorithm of \algref{alg:generalparallel} and \algref{alg:generalparallel:commit}.
Unlike in \hogwild{}, the concurrency control mechanisms of \occ{} ensure that concurrent transactions are serialized when they are not independent.

Serializability is achieved by maintaining sets $\hat{A}$, $\tilde{A}$, $\hat{B}$, $\tilde{B}$, which serve as upper \emph{and} lower bounds on the true state of $A$ and $B$ at commit time.
Each thread can determine locally if a decision to include or exclude an element can be taken safely.
Otherwise, the proposal is deferred to the commit process (\algref{alg:occcommit}) which waits until it is certain about $A$ and $B$ before proceeding.

The commit order is given by $\iota(e)$, which is the value of $\iota$ at line \ref{alg:occ:time} of \algref{alg:occ}.
We define $A^{\iota(e)-1}$, $B^{\iota(e)-1}$ as before with \hogwild{}.
Additionally, let $\hat{A}_e$, $\hat{B}_e$, $\tilde{A}_e$, and $\tilde{B}_e$ be the sets that are returned by \algref{alg:occsnapshot}\footnoteref{fn:copyvsshared}.
Indeed, these sets are guaranteed to be bounds on $A^{\iota(e)-1}$, $B^{\iota(e)-1}$:

\begin{restatable}{lem}{lemoccsetbound}\label{lem:occ:set_bound}
In \occ{}, $\forall e\in V$,
$\hat{A}_e \subseteq A^{\iota(e)-1} \subseteq \tilde{A}_e \backslash e$, and $\hat{B}_e \supseteq B^{\iota(e)-1} \supseteq \tilde{B}_e \cup e$.
\end{restatable}
Intuitively, these bounds are maintained by recording potential effects of concurrent transactions in $\tilde{A}$, $\tilde{B}$, and only recording the actual effects in $\hat{A}$, $\hat{B}$;
we leave the full proof to Appendix \ref{app:algoproof}.
Furthermore, by committing transactions in order $\iota$, we have $\hat{A} = A^{\iota(e)-1}$ and $\hat{B} = B^{\iota(e)-1}$ during commit.
% Furthermore, since later transactions are blocked, we have $\hat{A} = A^{\iota(e)-1}$ and $\hat{B} = B^{\iota(e)-1}$ at commit.

\begin{restatable}{lem}{lemoccdefer}\label{lem:occ:defer}
In \occ{}, when committing element $e$, we have $\hat{A} = A^{\iota(e)-1}$ and $\hat{B} = B^{\iota(e)-1}$.
\end{restatable}

% \algref{alg:occ} computes
% \begin{align*}
%   \Delta_+^{\min}(e) &= F(\tilde{A}_e) - F(\tilde{A}_e \backslash e),
% & \Delta_+^{\max}(e) &= F(\hat{A}_e \cup e) - F(\hat{A})\\
%   \Delta_-^{\min}(e) &= F(\tilde{B}_e) - F(\tilde{B}_e \cup e),
% & \Delta_-^{\max}(e) &= F(\hat{B}_e \backslash e) - F(\hat{B}).
% \end{align*}

\begin{cor}\label{cor:occ:delta_bound} Submodularity of $F$ implies that the $\Delta$'s computed by \occ{} satisfy $\Delta_+^{\min}(e) \leq \Delta_+^{\text{exact}}(e) = \Delta_+(e) \leq \Delta_+^{\max}(e)$ and $\Delta_-^{\min}(e) \leq \Delta_-^{\text{exact}}(e) = \Delta_-(e) \leq \Delta_-^{\max}(e)$.
% \begin{align}
% \Delta_+^{\min}(e) \leq \Delta_+(e)
% % = \Delta_+^{\text{exact}}(e)
% & \leq \Delta_+^{\max}(e) \\
% \Delta_-^{\min}(e) \leq \Delta_-(e)
% % = \Delta_+^{\text{exact}}(e)
% & \leq \Delta_-^{\max}(e).
% \end{align}
\end{cor}

By using these bounds, \occ{} can determine when it is safe to construct the transaction locally.
For failed transactions, the server is able to construct the correct transaction using the true program state.
As a consequence we can guarantee that the parallel execution of \occ{} is serializable.














\section{Analysis of Algorithms \label{sec:analysis}}

% \xinghao{What is the purpose of this section?
% We want to say that we have a choice between a slower but serially equivalent algorithm and a faster algorithm which is not serially equivalent.
% Nevertheless, the slower algorithm is not too slow, and the approximation guarantee of the algorithm that is not serially equivalent is not too weak.}

Our two algorithms trade off performance and strong approximation guarantees.
The \hogwild{} algorithm emphasizes speed at the expense of the approximation objective.
On the other hand, \occ{} emphasizes the tight $1/2$-approximation at the expense of increased coordination.
In this section we characterize the reduction in the approximation objective as well as the increased coordination.

%is a scalable algorithm that guarantees the tight 1/2 approximation.


\subsection{Approximation of \hogwild{} double greedy \label{sec:analysis:hogwild}}
% Let $F$ be submodular and non-negative.
% We assume for each $e$, the random variable $\rho_e := \max\{\Delta_+^{\max}(e) - \Delta_+(e), \Delta_-^{\max}(e) - \Delta_-(e)\}$ is bounded.
% This is trivially possible since
% \begin{align*}
% \rho_e
% &&\leq&&& \max_{S,T\subseteq V} \{[F(S\cup e) - F(S)] - [F(S \cup T \cup e) - F(S \cup T)]\}
% &\leq&& F(e)\kappa_F
% \end{align*}
% where $\kappa_F$ is the total curvature of $F$.
% % Summing over $e$ then gives us $\sum_e \rho_e \leq \kappa_F\sum_e F(e)$.

\begin{restatable}{thm}{thmrandomapprox}\label{thm:randomapprox} Let $F$ be a non-negative
%(monotone or non-monotone)
submodular function.
\hogwild{} solves the unconstrained problem $\max_{A\subset V} F(A)$ with worst-case approximation factor
$
E[F(A_{\hogwildshort{}})] \geq \frac{1}{2}F^* - \frac{1}{4}\sum_{i=1}^N E[\rho_i]$,
where $A_{\hogwildshort{}}$ is the output of the algorithm, $F^*$ is the optimal value, and $\rho_i = \max\{\Delta_+^{\max}(e) - \Delta_+(e), \Delta_-^{\max}(e) - \Delta_-(e)\}$ is the maximum discrepancy in the marginal gain due to the bounds.
\end{restatable}

The proof (\appendref{app:proofhogwild}) of \thmref{thm:randomapprox} follows the structure in \cite{buchbinder2012}.  \thmref{thm:randomapprox} captures the deviation from optimality as a function of width of the bounds which we characterize for two common applications. % of submodular maximization.
%We characterize $E[\rho_i]$ for two applications.


\textbf{Example: max graph cut.}
For the max cut objective we bound the expected discrepancy in the marginal gain $\rho_i$ in-terms of the sparsity of the graph and the maximum between processor message delay $\tau$.
By applying \thmref{thm:randomapprox} we obtain the approximation factor $E[F(A^N)] \geq \frac{1}{2} F(OPT) - \tau\frac{\#\text{edges}}{2N}$ which decreases linearly in both the message delays and graph density.
% Assuming a bounded message delay of $\tau$ and edges with unit weight, we can bound the expected discrepancy $\sum_i E[\rho_i] \leq 2\tau\frac{\text{\#edges}}{2N}$.
% The approximation of \hogwild{} is then $E[F(A^N)] \geq = \frac{1}{2} F(OPT) - \tau\frac{\#\text{edges}}{2N}$.
% In sparse graphs, \hogwild{} is off by a small additional term, which albeit grows linearly in $\tau$.
In a complete graph, $F^* = \frac{1}{2}\#\text{edges}$, so $E[F(A^N)] \geq F^*\left(\frac{1}{2} - \frac{\tau}{N}\right)$, which makes it possible to scale $\tau$ linearly with $N$ while retaining the same approximation factor.


\textbf{Example: set cover.}
Consider the simple set cover function,
$F(A) = \sum_{l=1}^L \min(1,|A\cap S_l|) - \lambda|A| = |\{l: A\cap S_l \neq\emptyset\}| - \lambda|A|$,
with $0 < \lambda \leq 1$.
We assume that there is some bounded delay $\tau$.
Suppose also the $S_l$'s form a partition, so each element $e$ belongs to exactly one set.
Then, $\sum_e E[\rho_e] \geq \tau + L(1-\lambda^\tau)$, which is linear in $\tau$ but independent of $N$.



\subsection{Correctness of \occ{}}

% \begin{restatable}{thm}{thmoccserializable} \occ{} is serializable.
% \end{restatable}


% \begin{thm} Let $F$ be a non-negative %(monotone or non-monotone)
% submodular function.
% \occ{} solves the unconstrained problem $\max_{A\subset V} F(A)$ with approximation
% $E[F(A_{\occshort{}})] \geq \frac{1}{2}F^*$,
% where $A_{\occshort{}}$ is the output of the algorithm, and $F^*$ is the optimal value.
% \end{thm}

\begin{restatable}{thm}{thmoccserializable}\label{thm:serializable}
\occ{} is serializable and therefore
% Let $F$ be a non-negative %(monotone or non-monotone)
% submodular function.
solves the unconstrained submodular maximization problem $\max_{A\subset V} F(A)$ with approximation
$E[F(A_{\occshort{}})] \geq \frac{1}{2}F^*$,
where $A_{\occshort{}}$ is the output of the algorithm, and $F^*$ is the optimal value.
\end{restatable}

The key challenge in the proof (\appendref{app:occserializable}) of \thmref{thm:serializable} is to demonstrate that \occ{} guarantees a serializable execution.
% We prove the serializability by induction on the commit order of elements.
It suffices to show that \occ{} takes the same decision as \seqalg{} for each element -- locally if it is safe to do so, and otherwise deferring the computation to the server.
%Full details of the proof are given in Appendix \ref{app:occserializable}.
As an immediate consequence of serializability, we recover the optimal approximation guarantees of the serial \seqalg{} algorithm.
% are preserved by \occ{}, including the approximation guarantees:




\subsection{Scalability of \occ{}}
Whenever a transaction is reconstructed on the server, the server needs to wait for all earlier elements to be committed, and is also blocked from committing all later elements.
Each failed transaction effectively constitutes a barrier to the parallel processing.
Hence, the scalability of \occ{} is dependent on the number of failed transactions.

For the max-cut and set cover example problems we can directly bound the number of failed transactions (details in \appendref{app:proofocc}).
For the max-cut problem with with a maximum inter processor message delay $\tau$ we obtain the upper bound $2 \tau \frac{\#edges}{N}$.
Similarly for for set cover the expected \emph{number} of failed transactions is upper bounded by $2\tau$.
As a consequence, the coordination costs of \occ{} grows at the same rate as the reduction in accuracy of \hogwild{}.


% Assume that there is a bounded delay $\tau$.
% The expected \emph{fraction} of failed transactions is upper bounded by $2\tau \frac{\#edges}{N^2}$.

% \textbf{Example: set cover.}
% Assuming a bounded delay $\tau$ and non-overlapping sets $S_l$'s, the expected \emph{number} of failed transactions is upper bounded by $2\tau$.


% \textbf{Example: max graph cut.}
% Assume that there is a bounded delay $\tau$.
% The expected \emph{fraction} of failed transactions is upper bounded by $2\tau \frac{\#edges}{N^2}$.

% \textbf{Example: set cover.}
% Assuming a bounded delay $\tau$ and non-overlapping sets $S_l$'s, the expected \emph{number} of failed transactions is upper bounded by $2\tau$.
















\section{Evaluation \label{sec:evaluation}}

We implemented the parallel and sequential double greedy algorithms in Java / Scala.
Experiments were conducted on Amazon EC2 using one cc2.8xlarge machine, up to 16 threads, for 10 repetitions.
We measured the runtime and speedup (ratio of runtime on 1 thread to runtime on $p$ threads).
For \hogwild{}, we measured $F(A_{\hogwildshort})-F(A_{\seqalgshort})$, the difference between the objective value on the sets returned by \hogwild{} and \seqalg{}.
We verified the correctness of \occ{} by comparing the output of \occ{} with \seqalg{}.
We also measured the fraction of transactions that fail in \occ{}.

%\begin{table}[h]
%\centering
%\begin{tabular}{|c|c|c|c|}\hline
%Graph & \# vertices & \# undirected edges & \# directed edges\\\hline\hline
%Google       &    875,713 &     3,852,985 &   4,563,235\\\hline
%BerkStan     &    685,230 &     6,649,470 &   7,600,595\\\hline
%Live Journal &  3,997,962 &    40,592,387 &  40,592,387\\\hline
%Orkut        &  3,072,441 &   117,181,608 & 117,181,608\\\hline
%Friendster   &  8,000,000 &   239,730,456 & 239,730,456\\\hline
%Friendster   & 10,000,000 &   625,279,786 & 625,279,786\\\hline
%Arabic-2005  & 22,744,080 & 1,107,806,146 & 631,153,669\\\hline
%UK-20005     & 39,459,925 & 1,566,054,250 & 921,345,078\\\hline
%\end{tabular}
%\end{table}
%\end{comment}

\begin{table}[h]
\centering\footnotesize
\begin{tabular}{|c|c|c|c|}\hline
Graph & \# vertices & \# edges & Description \\\hline\hline
Erdos-Renyi & 20,000,000 & $\approx 2 \times 10^6$ & Each edge is included with probability $5\times 10^{-6}$.\\\hline
\multirow{3}{*}{ZigZag}      & \multirow{3}{*}{25,000,000} &  \multirow{3}{*}{2,025,000,000} & Expander graph. The 81-regular zig-zag product \\
& & & between the Cayley graph on $\mathbb{Z}_{2500000}$ with generating \\
& & & set $\{\pm 1,\dots,\pm 5\}$, and the complete graph $K_{10}$.\\\hline
Friendster  & 10,000,000 &    625,279,786 & Subgraph of social network. \cite{snap}\\\hline
Arabic-2005 & 22,744,080 &    631,153,669 & 2005 crawl of Arabic web sites \cite{BoVWFI, BRSLLP, BCSU3}. \\\hline
UK-2005     & 39,459,925 &    921,345,078 & 2005 crawl of the .uk domain \cite{BoVWFI, BRSLLP, BCSU3}. \\\hline
IT-2004     & 41,291,594 &  1,135,718,909 & 2004 crawl of the .it domain \cite{BoVWFI, BRSLLP, BCSU3}. \\\hline
\end{tabular}
\caption{\footnotesize Synthetic and real graphs used in the evaluation of our parallel algorithms.}
\label{tab:graphstats}
\end{table}



We tested our parallel algorithms on the max graph cut and set cover problems with two synthetic graphs and three real datasets (Table \ref{tab:graphstats}).
% Graphs were pre-processed to remove self-loops.
We found that vertices were typically indexed such that nearby vertices in the graph were also close in their indices.
To reduce this dependency, we randomly permuted the ordering of vertices.
% For the max graph cut problem, we removed directions on edges to obtain undirected graphs.
% The set cover problem is reduced to a vertex cover on the directed graph.


\begin{figure}[ht]
  \centering
  \begin{tabular}{cccc}
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_relruntime.pdf}
			\caption{}
			\label{fig:relruntime}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_speedup_maxgraphcut.pdf}
			\caption{}
			\label{fig:speedup_maxgraphcut}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_speedup_setcover.pdf}
			\caption{}
			\label{fig:speedup_setcover}
	  \end{subfigure} \\
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_diffFA_maxgraphcut.pdf}
			\caption{}
			\label{fig:difffa_maxgraphcut}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_diffFA_setcover.pdf}
			\caption{}
			\label{fig:difffa_setcover}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_validated_maxgraphcut.pdf}
			\caption{}
			\label{fig:validated_maxgraphcut}
	  \end{subfigure} \\
  \end{tabular}
  \caption{\footnotesize Experimental results.
  \figref{fig:relruntime} -- runtime of the parallel algorithms as a ratio to that of the sequential algorithm. Each curve shows the runtime of a parallel algorithm on a particular graph for a particular function $F$.
  \figref{fig:speedup_maxgraphcut}, \ref{fig:speedup_setcover} -- speedup (ratio of runtime on one thread to that on $p$ threads).
  \figref{fig:difffa_maxgraphcut}, \ref{fig:difffa_setcover} -- \% difference between objective values of \seqalg{} and \hogwild{}, i.e. $[F(A_{\hogwildshort{}}) / F(A_{\seqalgshort{}}) - 1] \times 100\%$.
  \figref{fig:validated_maxgraphcut} -- percentage of transactions that fail in \occ{} on the max graph cut problem.
  }
\label{fig:results_quality}
\end{figure}


% Due to space constraints, we only present part of our results in \figref{fig:results_quality}, deferring full results to Appendix \ref{app:exptresults}.
We summarize of the key results here with more detailed experiments and discussion in \appendref{app:exptresults}.
\textbf{Runtime, Speedup:} Both parallel algorithms are faster than the sequential algorithm with three or more threads, and show good speedup properties as more threads are added ($\sim$ 10x or more for all graphs and both functions).
\textbf{Objective value:} The objective value of \hogwild{} decreases with the number of threads, but differs from the sequential objective value by less than $0.01\%$.
\textbf{Failed transactions:} \occ{} fails more transactions as threads are added, but even with 16 threads, less than 0.015\% transactions fail, which has negligible effect on the runtime / speedup.

\subsection{Adversarial ordering}

To highlight the differences in approaches between the two parallel algorithms, we conducted experiments on a ring Cayley expander graph on $\mathbb{Z}_{10^6}$ with generating set $\{\pm 1,\dots, \pm 1000\}$.
The algorithms are presented with an adversarial ordering, without permutation, so vertices close in the ordering are adjacent to one another, and tend to be processed concurrently.
This causes \hogwild{} to make more mistakes, and \occ{} fail more transactions.
% \xinghao{These experiments were conducted by using an atomic integer to select elements to process. We could instead use a partitioning scheme, which has 2 advantages. Firstly, there is less coordination -- for \hogwild{}, we essentially have no coordination.
% Secondly, when faced with an adversarial ordering, the partitioning scheme allows big jumps / re-orderings, which reduces the number of validations and \textit{increases} the objective value of both parallel algorithms.}
While more sophisticated partitioning schemes could improve scalability and eliminate the effect of adversarial ordering, we use the default data partitioning in our experiments to highlight the differences between the two algorithms.
% \footnote{
% We point out by using a partitioning scheme, it is possible to avoid the problems caused by the adversarial ordering, and to improve scalability.
% Nevertheless, we present results that do \emph{not} use the partitioning scheme, so as to better highlight the differences between the two parallel algorithms.
% }
As \figref{fig:results_adversarial} shows, \occ{}  sacrifices speed to ensure serial equivalence, eventually failing on $>90\%$ of transactions.
On the other hand, \hogwild{} focuses on speed, resulting in faster runtime, but achieves an objective value that is $20\%$ of $F(A_{\seqalgshort{}})$.
% For the set cover problem, we maintain statistics that are updated atomically by both algorithms.
% The adversarial ordering forces more concurrent atomic updates, and hence, \hogwild{} does not achieve good speedup.
% \footnote{We could have reduced coordination by computing $F$ directly, but doing so would result in longer runtimes.}

\begin{figure}[t]
  \centering
  \begin{tabular}{cccc}
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/runtime_ring_setcover.pdf}
			\caption{}
			\label{fig:runtime_ring_setcover}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/speedup_ring_setcover.pdf}
			\caption{}
			\label{fig:speedup_ring_setcover}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/validateddiffFA_ring_setcover.pdf}
			\caption{}
			\label{fig:validateddiffFA_ring_setcover}
	  \end{subfigure} \\
  \end{tabular}
  \caption{\footnotesize Experimental results for set cover problem on
  a ring expander graph demonstrating that for adversarially constructed inputs we can reduce the optimality of \hogwild{} and increase coordination costs for \occ{}.}
\label{fig:results_adversarial}
\end{figure}










\section{Related Work \label{sec:related}}
\textbf{Similar approach: }
Coordination-free solutions have been proposed for stochastic gradient descent \cite{Recht11} and collapsed Gibbs sampling \cite{Ahmed12}.
More generally, parameter servers \cite{li2013, ho2013} apply the \hogwildshort{} approach to larger classes of problems.
\cite{pan2013} applied concurrency control to parallelize some unsupervised learning algorithms.
\textbf{Similar problem: } \cite{Mirzasoleiman2013} explored distributed greedy submodular maximization, but only applied to monotone functions.






\section{Conclusion and Future Work \label{sec:discussions}}

By adopting the transaction processing model from parallel database systems, we presented two approaches to parallelizing the double greedy algorithm for unconstrained submodular maximization.
We quantified the weaker approximation guarantee of \hogwild{} and the additional coordination of \occ{}, allowing one to trade off between performance and objective optimality.
Our evaluation on large scale data demonstrates the scalability and tradeoffs of the two approaches.
Moreover, as the approximation quality of the \hogwild{} algorithm decreases so does the scalability of the \occ{} algorithm.
The choice between the algorithm then reduces to a choice of guaranteed performance and guaranteed optimality.

We believe there are a number of areas for future work.
One can imagine a system that allows a smooth interpolation between \hogwild{} and \occ{}.
While both \hogwild{} and \occ{} can be immediately implemented as distributed algorithms, higher communication costs and delays may pose additional challenges.
Finally, other problems such as constrained maximization of monotone / non-monotone functions could potentially be parallelized with the \hogwildshort{} and \occshort{} frameworks.

%\newpage
{\footnotesize
%\subsection*{Acknowledgments}
%This research is supported in part by NSF CISE Expeditions award CCF-1139158 and DARPA XData Award FA8750-12-2-0331, and  gifts from Amazon Web Services, Google, SAP,  Blue Goji, Cisco, Clearstory Data, Cloudera, Ericsson, Facebook, General Electric, Hortonworks, Intel, Microsoft, NetApp, Oracle, Samsung, Splunk, VMware and Yahoo!.
%This material is also based upon work supported in part by the Office of
%Naval Research under contract/grant number N00014-11-1-0688.
%X. Pan's work is also supported in part by a DSO National Laboratories Postgraduate Scholarship.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliographystyle{unsrtnat}
\bibliography{references_arxiv}
}

\newpage
\appendix



% \input{sepsums}

\newpage\input{algoproofs}

\newpage\input{hogwildproof}

\newpage\input{occproof}

\newpage\section{Lemma}
\begin{lem}\label{lem:sumbinomial} $\sum_{k=t}^{a-b+t} {k-j \choose t-j} {a-k+j \choose b-t+j} = {a+1 \choose b+1}$.
\end{lem}
\begin{proof}
\begin{align*}
&\sum_{k=t}^{a-b+t} {k-j \choose t-j} {a-k+j \choose b-t+j}\\
&= \sum_{k'=0}^{a-b} {k'+t-j \choose t-j} {a-k'-t+j \choose b-t+j} \\
&= \sum_{k'=0}^{a-b} {k'+t-j \choose k'} {a-k'-t+j \choose a-b-k'} & \text{(symmetry of binomial coeff.)}\\
&= (-1)^{a-b}\sum_{k'=0}^{a-b} {-t+j-1 \choose k'} {-b+t-j-1 \choose a-b-k'} & \text{(upper negation)}\\
&= (-1)^{a-b} {-b-2 \choose a-b} & \text{(Chu-Vandermonde's identity)}\\
&= {a+1 \choose a-b} & \text{(upper negation)}\\
&= {a+1 \choose b+1} & \text{(symmetry of binomial coeff.)}\\
\end{align*}
\end{proof}

\newpage\input{exptresults}

\newpage\input{examples}

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz,
% slightly modified from the 2009 version by Kiri Wagstaff and
% Sam Roweis's 2008 version, which is slightly modified from
% Prasad Tadepalli's 2007 version which is a lightly
% changed version of the previous year's version by Andrew Moore,
% which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
